{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "eb9778d6-d74d-4312-83c0-09673e0a5d6d",
      "metadata": {
        "id": "eb9778d6-d74d-4312-83c0-09673e0a5d6d"
      },
      "source": [
        "# Про курс"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa6b0dfd-7c36-433c-804e-230279824658",
      "metadata": {
        "id": "fa6b0dfd-7c36-433c-804e-230279824658"
      },
      "source": [
        "Курс будет опираться на курс осеннего семестра \"Машинное обучение в биологии\", но отдельные темы повторим.\n",
        "\n",
        "Лекции проводятся по средам, 19:00 по Москве, в удаленном формате.\n",
        "Семинары тоже по средам, очно, чуть раньше.\n",
        "\n",
        "Домашки будет две, постараемся провести их в Kaggle."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c118d38c-ae68-44c7-b171-c97ec277f259",
      "metadata": {
        "id": "c118d38c-ae68-44c7-b171-c97ec277f259"
      },
      "source": [
        "## План \"Максимум\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b6e622b-5df9-4817-ad20-84da14f660b4",
      "metadata": {
        "id": "0b6e622b-5df9-4817-ad20-84da14f660b4"
      },
      "source": [
        "- Block 1. Classic NLP and Retrieval\n",
        "\t- Text Feature Extraction. Tokenization, stemming, lemmatization, BPE, **TF-IDF**, BM25.\n",
        "\t- **Word2Vec**: Continuous Bag-of-Words and Skip-Gram. Revisiting transformers: Encoders vs Decoders vs Full Transformers. BERT family, GPTs, T5.\n",
        "\t- Vector Search. Algorithms for fast nearest neighbors search.\n",
        "- Block 2. Graphs\n",
        "\t- Classical graph methods. Centrality, clustering, DeepWalk, Node2Vec and Matrix decomposition.\n",
        "\t- Graph Neural Networks. **GCN**, GAT (revisit). Heterogeneous Graphs, Knowledge Graphs, KG Decoders (TransE, ComplEx), RGCN.\n",
        "\t- Proteins. GNN in 3D, equivariance in deep learning. **AlphaFold 2**.\n",
        "- Block 3. Reinforcement Learning\n",
        "\t- Intro to RL. Markov Decision Processes. Policy and Value. **Q-learning**. DQN.\n",
        "\t- Policy Gradient Methods. REINFORCE, Actor-Critic. **PPO**\n",
        "- Block 4. Generative AI\n",
        "\t- LLM Applications. Sampling Methods, **RAG**, **Agents**, Chain-of-Thoughts and Co.\n",
        "\t- **Diffusion**. Generating images. *Schedules*.\n",
        "\t- Diffusion for chemical compounds, 3D structures (proteins and peptides). *Latent Diffusion Models*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c2fbd31-b445-4163-b75c-a004661acb50",
      "metadata": {
        "id": "2c2fbd31-b445-4163-b75c-a004661acb50"
      },
      "source": [
        "# Представления слов"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a48c93e-5d95-4dfc-9b5e-d08ade545c41",
      "metadata": {
        "id": "7a48c93e-5d95-4dfc-9b5e-d08ade545c41"
      },
      "source": [
        "То, как модели машинного обучения \"видят\" данные, отличается от того, как это делаем мы. Например, мы можем легко понять текст \"ЩУКа съела ацетат\", но наши модели не могут этого сделать - им нужно какое-то представление данных, скорее всего в виде какого-то *вектора*. Такие векторы, или **эмбеддинги**, представляют собой представления слов, которые могут быть поданы в вашу модель.\n",
        "\n",
        "Векторы полезны при работе с любыми текстами, в том числе биологическими. Векторы остатков аминокислот, нуклеиновых кислот очень часто используются.\n",
        "\n",
        "![image](https://lena-voita.github.io/resources/lectures/word_emb/word_repr_intro-min.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c66f6326-4b23-49f3-97e9-c32e33798d5e",
      "metadata": {
        "id": "c66f6326-4b23-49f3-97e9-c32e33798d5e"
      },
      "source": [
        "На практике у вас есть словарь разрешенных слов, который вы выбираете заранее. Для каждого слова в словаре есть таблица поиска, содержащая его эмбеддинги.\n",
        "\n",
        "Для учета неизвестных слов (тех, которых нет в словаре) в словаре обычно используется специальная лексема **UNK**. В качестве альтернативы неизвестные лексемы можно игнорировать или присваивать им нулевой вектор.\n",
        "\n",
        "Главный вопрос: как мы получаем эти векторы слов?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa2132b6-903d-468c-b24e-7740e5a0741e",
      "metadata": {
        "id": "aa2132b6-903d-468c-b24e-7740e5a0741e"
      },
      "source": [
        "## One-hot encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6d4a65c-d330-4613-bb39-b53884d166ee",
      "metadata": {
        "id": "b6d4a65c-d330-4613-bb39-b53884d166ee"
      },
      "source": [
        "Самый простой способ - представить слова в виде векторов с одной точкой: для $i$-го слова в словаре вектор имеет 1 в $i$-м измерении и 0 в остальных. В машинном обучении это самый простой способ представления категориальных признаков.\n",
        "\n",
        "![image](https://lena-voita.github.io/resources/lectures/word_emb/one_hot-min.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8afbc5f4-20f7-4bd6-8ee9-27294feb51a1",
      "metadata": {
        "id": "8afbc5f4-20f7-4bd6-8ee9-27294feb51a1"
      },
      "source": [
        "Нетрудно догадаться, почему такиe векторы - не лучший способ представления слов. Одна из проблем заключается в том, что для больших словарей эти векторы будут очень длинными: размерность вектора равна размеру словаря. На практике это нежелательно, но данная проблема не является самой важной.\n",
        "\n",
        "На самом например, одноточечные векторы \"думают\", что кошка так же близка к собаке, как и стол. Можно сказать, что одноточечные векторы не улавливают смысл."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdf18bc2-c747-4e98-8219-dab0172ad245",
      "metadata": {
        "id": "fdf18bc2-c747-4e98-8219-dab0172ad245"
      },
      "source": [
        "## Дистрибутивная семантика"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19918610-2e36-43c6-9fcc-5544a3b9b23b",
      "metadata": {
        "id": "19918610-2e36-43c6-9fcc-5544a3b9b23b"
      },
      "source": [
        "Чтобы отразить смысл слов в их векторах, нам сначала нужно определить понятие смысла, которое можно использовать на практике.\n",
        "\n",
        "Часто возможно извлекать смысл из **контекста**. В этом идея всех методов извлечения признаков из слов.\n",
        "\n",
        "Самые простые из них используют то или иное матричное разложение."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56d5aabd-da33-4e9a-868e-2457896834e3",
      "metadata": {
        "id": "56d5aabd-da33-4e9a-868e-2457896834e3"
      },
      "source": [
        "### Count-based methods"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3888d0c2-5ed1-4cd9-b608-5f73802b403b",
      "metadata": {
        "id": "3888d0c2-5ed1-4cd9-b608-5f73802b403b"
      },
      "source": [
        "![image](https://lena-voita.github.io/resources/lectures/word_emb/preneural/idea-min.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d7ecbc9-4677-4245-a886-e332a926e2ea",
      "metadata": {
        "id": "3d7ecbc9-4677-4245-a886-e332a926e2ea"
      },
      "source": [
        "Общая процедура показана выше и состоит из двух шагов:\n",
        "\n",
        "1. построить матрицу \"слово-контекст\"\n",
        "2. уменьшить ее размерность.\n",
        "\n",
        "Уменьшение размерности необходимо по двум причинам. Во-первых, исходная матрица очень велика. Во-вторых, поскольку многие слова встречаются лишь в нескольких возможных контекстах, матрица потенциально содержит много неинформативных элементов (по сути нулей).\n",
        "\n",
        "Чтобы оценить сходство между словами/контекстами, обычно требуется оценить точечный произведение нормализованных векторов слов/контекстов (т.е. косинусное сходство)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28d0be37-efd9-4ec0-8c86-8ac09fd832f2",
      "metadata": {
        "id": "28d0be37-efd9-4ec0-8c86-8ac09fd832f2"
      },
      "source": [
        "Чтобы определить метод, основанный на подсчете, нам нужны две вещи:\n",
        "\n",
        "1. возможные контексты (в том числе нужно знать, что означает появление слова в том или ином контексте)\n",
        "2. понятие ассоциации, то есть формулы для вычисления элементов матрицы.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc6a08d8-363a-44e2-a598-fff3b070c1d3",
      "metadata": {
        "id": "bc6a08d8-363a-44e2-a598-fff3b070c1d3"
      },
      "source": [
        "### Co-Occurence Counts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c77dc75-2b17-4087-967a-9650e84c128d",
      "metadata": {
        "id": "4c77dc75-2b17-4087-967a-9650e84c128d"
      },
      "source": [
        "![image](https://lena-voita.github.io/resources/lectures/word_emb/preneural/window-min.png)\n",
        "\n",
        "Самый простой подход - определить контексты как каждое слово в окне размером $L$. Элемент матрицы для пары слово-контекст ($w$, $c$) - это количество раз, когда $w$ встречается в контексте $c$. Это самый базовый (и очень, очень старый) метод получения эмбеддингов."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d6a0403-0892-4d0e-9a6a-e2a8d9c78809",
      "metadata": {
        "id": "6d6a0403-0892-4d0e-9a6a-e2a8d9c78809"
      },
      "source": [
        "### PPMI"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05628691-c23d-45a5-89e6-de380f703713",
      "metadata": {
        "id": "05628691-c23d-45a5-89e6-de380f703713"
      },
      "source": [
        "Здесь контексты определяются так же, как и раньше, но мера связи между словом и контекстом более умная: положительный PMI (или сокращенно PPMI). Мера PPMI широко рассматривается как лучшая до нейронок моделей.\n",
        "\n",
        "![image](https://lena-voita.github.io/resources/lectures/word_emb/preneural/define_ppmi-min.png)\n",
        "\n",
        "Было показано, что Word2Vec, с которым мы скоро познакомимся, неявно аппроксимирует факторизацию матрицы PMI."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e8049b1-f487-437e-a3c2-938fc48a8653",
      "metadata": {
        "id": "4e8049b1-f487-437e-a3c2-938fc48a8653"
      },
      "source": [
        "## Latent Semantic Analysis (LSA)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f98a5b7f-6a85-4aef-8d82-6ba855ce8009",
      "metadata": {
        "id": "f98a5b7f-6a85-4aef-8d82-6ba855ce8009"
      },
      "source": [
        "**LSA** анализирует коллекцию документов, не отдельные слова. Если в предыдущих подходах контексты служили только для получения векторов слов и затем отбрасывались, то здесь нас также интересует контекст, или, в данном случае, векторы документов.\n",
        "\n",
        "LSA - одна из простейших тематических моделей: для измерения сходства между документами можно использовать косинусоидальное сходство между векторами документов.\n",
        "\n",
        "![image](https://lena-voita.github.io/resources/lectures/word_emb/preneural/lsa-min.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word2Vec"
      ],
      "metadata": {
        "id": "bWHYuxbtntpr"
      },
      "id": "bWHYuxbtntpr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2Vec - это модель, параметрами которой являются векторы слов. Эти параметры итеративно оптимизируются для достижения определенной цели. Цель заставляет векторы слов \"знать\" контексты, в которых может встречаться слово: векторы обучаются предсказывать возможные контексты соответствующих слов. Как вы помните из гипотезы распределения, если векторы \"знают\" о контекстах, то они \"знают\" и значение слова."
      ],
      "metadata": {
        "id": "g4CwcQHMn26M"
      },
      "id": "g4CwcQHMn26M"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2Vec - это итерационный метод. Его основная идея заключается в следующем:\n",
        "\n",
        "- возьмите огромный корпус текстов;\n",
        "- пройдите по тексту с помощью скользящего окна, перемещаясь по одному слову за раз. На каждом шаге есть центральное слово и контекстные слова (другие слова в этом окне);\n",
        "- для центрального слова вычислите вероятности контекстных слов;\n",
        "\n",
        "![image](https://lena-voita.github.io/resources/lectures/word_emb/w2v/window_prob1-min.png)"
      ],
      "metadata": {
        "id": "XFd7oF5nn-xj"
      },
      "id": "XFd7oF5nn-xj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "На самом деле можно делать и иначе. Существует две разновидности word2vec моделей:\n",
        "\n",
        "* **Continuous bag-of-words model**: предсказывает среднее слово на основе окружающих контекстных слов. Контекст состоит из нескольких слов до и после текущего (среднего) слова. Эта архитектура называется моделью мешка слов, поскольку порядок слов в контексте не важен.\n",
        "* **Skip-gram**: предсказывает слова в определенном диапазоне до и после текущего слова в одном и том же предложении. Ниже приведен пример работы этой модели.\n",
        "\n",
        "\n",
        "Обе оптимизируют вероятнисти:\n",
        "\n",
        "![word2vec_full_softmax](https://tensorflow.org/text/tutorials/images/word2vec_full_softmax.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "R-ca1l1SoYn6"
      },
      "id": "R-ca1l1SoYn6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вычисление знаменателя этой формулы предполагает выполнение полного softmax по всему словарному запасу слов, который часто бывает большим: до десятков миллионов терминов."
      ],
      "metadata": {
        "id": "nHzNLdTiqIHb"
      },
      "id": "nHzNLdTiqIHb"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}