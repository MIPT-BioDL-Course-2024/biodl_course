{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ujun2a2kDxjJ"
      },
      "outputs": [],
      "source": [
        "%pip install -q transformers huggingface_hub\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Использование предварительно обученных трансформеров\n",
        "\n",
        "Существует множество инструментов, позволяющих получить доступ к предварительно\n",
        "обученным моделям трансформеров, но самым мощным и удобным из них является\n",
        "[`huggingface/transformers`](https://github.com/huggingface/transformers).\n",
        "На этой неделе на практике вы научитесь загружать, применять и модифицировать\n",
        "предварительно обученные трансформеры для различных задач.\n",
        "\n",
        "Пайплайны: если вы хотите просто применить предварительно обученную модель,\n",
        "вы можете сделать это одной строкой кода с помощью пайплайна.\n",
        "Huggingface/transformers предлагает выбор предварительно настроенных пайплайнов\n",
        "для маскированного моделирования языка, классификации сентиментов, ответов на\n",
        "вопросы и т. д.\n",
        "([см. полный список здесь](https://huggingface.co/transformers/main_classes/pipelines.html))\n",
        "\n",
        "Типичный пайплайн включает:\n",
        "\n",
        "* предварительную обработку, например, токенизацию, сегментацию на подслова\n",
        "* основную модель (например, BERT)\n",
        "* какую-то постобработку вывода\n",
        "\n",
        "Давайте посмотрим на это в действии:"
      ],
      "metadata": {
        "id": "_YHTm_cbGYK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "classifier = transformers.pipeline('sentiment-analysis', model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "print(classifier(\"BERT is amazing!\"))"
      ],
      "metadata": {
        "id": "aeg15OovFh58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "data = {\n",
        "    'arryn': 'As High as Honor.',\n",
        "    'baratheon': 'Ours is the fury.',\n",
        "    'stark': 'Winter is coming.',\n",
        "    'tyrell': 'Growing strong.'\n",
        "}\n",
        "\n",
        "# YOUR CODE: predict sentiment for each noble house and create outputs dict\n",
        "<...>\n",
        "outputs = <YOUR CODE: dict (house name) : True if positive, False if negative>\n",
        "\n",
        "assert sum(outputs.values()) == 3 and outputs[base64.decodebytes(b'YmFyYXRoZW9u\\n').decode()] == False\n",
        "print(\"Well done!\")"
      ],
      "metadata": {
        "id": "U9cPrS_bFkos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlm_model = transformers.pipeline('fill-mask', model=\"bert-base-uncased\")\n",
        "MASK = mlm_model.tokenizer.mask_token\n",
        "\n",
        "for hypo in mlm_model(f\"Donald {MASK} is the president of the United States.\"):\n",
        "    print(f\"P={hypo['score']:.5f}\", hypo['sequence'])\n",
        "# at least he was the president when the model was trained"
      ],
      "metadata": {
        "id": "tnfxTz0VFmTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Your turn: use bert to retrieve some information.\n",
        "# For example, you can try to find out where Cyt c is expressed.\n",
        "#The answers are rather disappointing.\n",
        "mlm_model(<PROMPT>)"
      ],
      "metadata": {
        "id": "hO4OyVVeFnbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Huggingface предлагает сотни предварительно обученных моделей, которые специализируются на различных задачах. Вы можете быстро найти нужную вам модель, используя [этот список](https://huggingface.co/models)."
      ],
      "metadata": {
        "id": "NLXnKX0BIOvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Almost two-thirds of the 1.5 million people who viewed this liveblog had Googled to discover\n",
        " the latest on the Rosetta mission. They were treated to this detailed account by the Guardian’s science editor,\n",
        " Ian Sample, and astronomy writer Stuart Clark of the moment scientists landed a robotic spacecraft on a comet\n",
        " for the first time in history, and the delirious reaction it provoked at their headquarters in Germany.\n",
        "  “We are there. We are sitting on the surface. Philae is talking to us,” said one scientist.\n",
        "\"\"\"\n",
        "\n",
        "# Task: create a pipeline for named entity recognition, use task name 'ner' and search for the right model in the list\n",
        "ner_model = <YOUR CODE>\n",
        "\n",
        "named_entities = ner_model(text)"
      ],
      "metadata": {
        "id": "4LRv6b8NFtw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('OUTPUT:', named_entities)\n",
        "word_to_entity = {item['word']: item['entity'] for item in named_entities}\n",
        "assert 'org' in word_to_entity.get('Guardian').lower() and 'per' in word_to_entity.get('Stuart').lower()\n",
        "print(\"All tests passed\")"
      ],
      "metadata": {
        "id": "qw4hCUKKIVIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Строительные блоки пайплайна\n",
        "\n",
        "Huggingface также позволяет вам получить доступ к своим пайплайнам на более низком уровне. Для этого есть две основные абстракции:\n",
        "* `Tokenizer` - преобразует строки в идентификаторы токенов (последовательности целых чисел) и обратно.\n",
        "* `Model` - pytorch `nn.Module` с предварительно обученными весами.\n",
        "\n",
        "Вы можете использовать такие Model в своем коде, в качестве частей еще больших моделей."
      ],
      "metadata": {
        "id": "Nes79TF9IxmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = transformers.AutoModel.from_pretrained('bert-base-uncased')\n"
      ],
      "metadata": {
        "id": "uZKJKlJoJDHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines = [\n",
        "    \"Luke, I am your father.\",\n",
        "    \"Life is what happens when you're busy making other plans.\",\n",
        "    ]\n",
        "\n",
        "# tokenize a batch of inputs. \"pt\" means [p]y[t]orch tensors\n",
        "tokens_info = tokenizer(lines, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "for key in tokens_info:\n",
        "    print(key, tokens_info[key])\n",
        "\n",
        "print(\"Detokenized:\")\n",
        "for i in range(2):\n",
        "    print(tokenizer.decode(tokens_info['input_ids'][i]))"
      ],
      "metadata": {
        "id": "9tKW2girJEBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can now apply the model to get embeddings\n",
        "with torch.no_grad():\n",
        "    out = model(**tokens_info)\n",
        "\n",
        "print(out['pooler_output'])"
      ],
      "metadata": {
        "id": "v6LSyuzjJFQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Трансформеростроение\n",
        "\n",
        "В этом разделе вы слой за слоем реализуете языковую модель, а затем используете ее для генерации (надеемся) связного текста.\n",
        "\n",
        "Сначала мы загрузим предварительно обученные веса для модели [GPT2 от OpenAI](https://openai.com/research/better-language-models) - известной модели 2019 года. Она достаточно маленькая, чтобы загрузить ее в колаб. Обучать ее, конечно, слишком долго. Поэтому и будем имплементировать модель, а затем заполнять ее уже готовыми весами."
      ],
      "metadata": {
        "id": "8N6Xj5FeJUJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "state_dict = torch.load(hf_hub_download(\"gpt2\", filename=\"pytorch_model.bin\"))\n",
        "for key, value in tuple(state_dict.items()):\n",
        "    if key.startswith('h.') and key.endswith('.weight') and value.ndim == 2:\n",
        "        value.transpose_(1, 0)  # <-- for compatibility with modern PyTorch modules\n",
        "    if key.startswith('h.') and key.endswith('.attn.bias') and value.ndim == 4:\n",
        "        state_dict.pop(key)  # <-- triangular binar masks, not needed in this code\n",
        "\n",
        "print('Weights:', repr(sorted(state_dict.keys()))[:320], '...')"
      ],
      "metadata": {
        "id": "w3UfWys6JuK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "В следующих нескольких ячейках мы будем реализовывать модель слой за слоем, чтобы использовать эти веса.\n",
        "\n",
        "Как вы помните, transformers содержат два основных типа слоев: слои внимания и полносвязные слои.\n",
        "\n",
        "Полносвязные слои гораздо проще для понимания, поэтому мы начнем с них:\n",
        "\n",
        "Пожалуйста, реализуйте полносвязные слой __без residual connection или layer normalization__ (мы добавим позже)."
      ],
      "metadata": {
        "id": "iHmnSHiRJ6DH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GeLUThatWasUsedInGPT2(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * x ** 3)))\n",
        "\n",
        "class FullyConnected(nn.Module):\n",
        "    def __init__(self, dim: int):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(dim, 4  * dim)\n",
        "        self.gelu = GeLUThatWasUsedInGPT2()\n",
        "        self.c_proj = nn.Linear(4 * dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x.shape = [batch_size, seq_length, dim]\n",
        "        <YOUR CODE HERE - COMPUTE LAYER OUTPUTS>\n",
        "        return <MLP OUTPUTS>\n"
      ],
      "metadata": {
        "id": "boB-XvW5KRYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь загрузим в слой веса модели:"
      ],
      "metadata": {
        "id": "H2l8yfNMKUSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlp = FullyConnected(dim=768)\n",
        "mlp.load_state_dict({'c_fc.weight': state_dict['h.0.mlp.c_fc.weight'],\n",
        "                     'c_fc.bias': state_dict['h.0.mlp.c_fc.bias'],\n",
        "                     'c_proj.weight': state_dict['h.0.mlp.c_proj.weight'],\n",
        "                     'c_proj.bias': state_dict['h.0.mlp.c_proj.bias']})\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "x = torch.randn(1, 2, 768)  # [batch_size, sequence_length, dim]\n",
        "checksum = torch.sum(mlp(x) * x)\n",
        "assert abs(checksum.item() - 1282.3315) < 0.1, \"layer outputs do not match reference\"\n",
        "assert torch.allclose(mlp(x[:, (1, 0), :])[:, (1, 0), :], mlp(x)), \"mlp must be permutation-invariant\"\n",
        "print(\"Seems legit!\")"
      ],
      "metadata": {
        "id": "iDfXPsbZKSZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attentions.\n",
        "\n",
        "Поскольку GPT-2 должен генерировать текст слева направо, каждая сгенерированный токен может обращать внимание только на лексемы слева (и на себя). Такой тип внимания называется *masked self-attention*, поскольку он маскирует токены справа.\n",
        "\n",
        "Реализуйте маскированное самовнимание __без residual connection или layer normalization__, как и в прошлый раз."
      ],
      "metadata": {
        "id": "qlr0TgWwKqRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedSelfAttention(nn.Module):\n",
        "    def __init__(self, dim: int, num_heads: int):\n",
        "        super().__init__()\n",
        "        self.c_attn = nn.Linear(dim, dim * 3)  # query + key + value, combined\n",
        "        self.c_proj = nn.Linear(dim, dim)  # output projection\n",
        "        self.dim, self.num_heads = dim, num_heads\n",
        "        self.head_size = dim // num_heads\n",
        "\n",
        "    def forward(self, x):\n",
        "        q, k, v = self.c_attn(x).split(dim=-1, split_size=self.dim)\n",
        "        assert q.shape == k.shape == v.shape == x.shape, \"q, k and v must have the same shape as x\"\n",
        "\n",
        "\n",
        "        # Note: this is an inefficient implementation that uses a for-loop.\n",
        "        # To get the full grade during homework, please re-implement this code:\n",
        "        # 1) do not use for-loops (or other loops). Compute everything in parallel with vectorized operations\n",
        "        # 2) do not use F.scaled_dot_product_attention - write your own attention code using basic PyTorch ops\n",
        "        head_outputs = []\n",
        "        for head_index in range(self.num_heads):\n",
        "            head_selector = range(self.head_size * head_index, self.head_size * (head_index + 1))\n",
        "\n",
        "            head_queries = q[..., head_selector]\n",
        "            head_keys = k[..., head_selector]\n",
        "            head_values = v[..., head_selector]\n",
        "\n",
        "            single_head_output = F.scaled_dot_product_attention(\n",
        "                <YOUR CODE HERE - fill in the missing parameters; see docs below>\n",
        "                is_causal=True)\n",
        "            # docs: https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html\n",
        "            head_outputs.append(single_head_output)\n",
        "\n",
        "        combined_head_outputs = torch.cat(head_outputs, dim=-1)\n",
        "        return self.c_proj(combined_head_outputs)\n"
      ],
      "metadata": {
        "id": "MSQxfLlPLA_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Протестируем"
      ],
      "metadata": {
        "id": "pQhUzj6iLCZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn = MaskedSelfAttention(dim=768, num_heads=12)\n",
        "attn.load_state_dict({'c_attn.weight': state_dict['h.0.attn.c_attn.weight'],\n",
        "                      'c_attn.bias': state_dict['h.0.attn.c_attn.bias'],\n",
        "                      'c_proj.weight': state_dict['h.0.attn.c_proj.weight'],\n",
        "                      'c_proj.bias': state_dict['h.0.attn.c_proj.bias']})\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "x = torch.randn(1, 10, 768)  # [batch_size, sequence_length, dim]\n",
        "checksum = torch.sum(attn(x) * x)\n",
        "assert abs(checksum.item() - 2703.6772) < 0.1, \"layer outputs do not match reference\"\n",
        "assert not torch.allclose(attn(x[:, (1, 0), :])[:, (1, 0), :], attn(x[:, (0, 1), :])), \"masked attention must *not* be permutation-invariant\"\n",
        "print(\"It works!\")"
      ],
      "metadata": {
        "id": "rOfP5NsXLDkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Собираем блоки вместе\n",
        "\n",
        "![img](https://i.imgur.com/1sq2vHO.png)"
      ],
      "metadata": {
        "id": "3t9QneymLCTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerLayer(nn.Module):\n",
        "    def __init__(self, dim: int, num_heads: int):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(dim)\n",
        "        self.attn = MaskedSelfAttention(dim, num_heads)\n",
        "        self.ln_2 = nn.LayerNorm(dim)\n",
        "        self.mlp = FullyConnected(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        <YOUR CODE - apply attention, mlp and layer normalization as shown in figure above>\n",
        "        return <...>"
      ],
      "metadata": {
        "id": "faIUneuULNFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer = TransformerLayer(dim=768, num_heads=12)\n",
        "layer.load_state_dict({k[5:]: v for k, v in state_dict.items() if k.startswith('h.10.')})\n",
        "assert abs(torch.sum(layer(x) * x).item() - 9874.7383) < 0.1\n",
        "print(\"Good job!\")"
      ],
      "metadata": {
        "id": "UzB4ozhNLN7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2(nn.Module):\n",
        "    def __init__(self, vocab_size: int, dim: int, num_heads: int, num_layers: int, max_position_embeddings: int = 1024):\n",
        "        super().__init__()\n",
        "        self.wte = nn.Embedding(vocab_size, dim)  # token embeddings\n",
        "        self.wpe = nn.Embedding(max_position_embeddings, dim)  # position embeddings\n",
        "        self.ln_f = nn.LayerNorm(dim)   # final layer norm - goes after all transformer layers, but before logits\n",
        "\n",
        "        self.h = nn.Sequential(*(TransformerLayer(dim, num_heads) for layer in range(num_layers)))\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        # input_ids.shape: [batch_size, sequence_length], int64 token ids\n",
        "        position_ids = torch.arange(input_ids.shape[1], device=input_ids.device).unsqueeze(0)\n",
        "\n",
        "        token_embeddings = self.wte(input_ids)\n",
        "        position_embeddings = self.wpe(position_ids)\n",
        "        full_embeddings = token_embeddings + position_embeddings\n",
        "\n",
        "        transformer_output = self.h(full_embeddings)\n",
        "        transformer_output_ln = self.ln_f(transformer_output)\n",
        "\n",
        "        # final layer: we predict logits by re-using token embeddings as linear weights\n",
        "        output_logits = transformer_output_ln @ self.wte.weight.T\n",
        "        return output_logits\n"
      ],
      "metadata": {
        "id": "z8ISM3GxLOyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained('gpt2', add_prefix_space=True)\n",
        "model = GPT2(vocab_size=50257, dim=768, num_heads=12, num_layers=12)\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "input_ids = tokenizer(\"A quick\", return_tensors='pt')['input_ids']\n",
        "\n",
        "predicted_logits = model(input_ids)\n",
        "most_likely_token_id = predicted_logits[:, -1].argmax().item()\n",
        "\n",
        "print(\"Prediction:\", tokenizer.decode(most_likely_token_id))"
      ],
      "metadata": {
        "id": "d9348BZPLP6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The Fermi paradox \"\n",
        "tokens = tokenizer.encode(text)\n",
        "print(end=tokenizer.decode(tokens))\n",
        "line_length = len(tokenizer.decode(tokens))\n",
        "\n",
        "for i in range(500):\n",
        "    # Predict logits with your model\n",
        "    with torch.no_grad():\n",
        "        logits = model(torch.as_tensor([tokens]))\n",
        "\n",
        "    # Sample with probabilities\n",
        "    p_next = torch.softmax(logits[0, -1, :], dim=-1).data.cpu().numpy()\n",
        "    next_token_index = np.random.choice(len(p_next), p=p_next)\n",
        "\n",
        "    tokens.append(int(next_token_index))\n",
        "    print(end=tokenizer.decode(tokens[-1]))\n",
        "    line_length += len(tokenizer.decode(tokens[-1]))\n",
        "    if line_length > 120:\n",
        "      line_length = 0\n",
        "      print()\n",
        "\n"
      ],
      "metadata": {
        "id": "itVHtSF5LRHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Как все то же сделать с transformers"
      ],
      "metadata": {
        "id": "k8GP8XBnLW2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained('gpt2', add_prefix_space=True)\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained('gpt2')\n",
        "print('Generated text:', tokenizer.decode(\n",
        "    model.generate(\n",
        "        **tokenizer(\"The Fermi paradox \", return_tensors='pt'),\n",
        "        do_sample=True, max_new_tokens=50\n",
        "    ).flatten().numpy()\n",
        "))\n"
      ],
      "metadata": {
        "id": "WSsPgTBULLx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Proteins"
      ],
      "metadata": {
        "id": "56jHwcJCLhTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Напоследок, посмотрим немного на модель биологической последовательности"
      ],
      "metadata": {
        "id": "Z2KtrpJ0NPAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel, BertTokenizer, BertConfig\n",
        "import re"
      ],
      "metadata": {
        "id": "ZUJJ6vqaNYzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert_bfd\", do_lower_case=False )\n",
        "model = BertModel.from_pretrained(\"Rostlab/prot_bert_bfd\")"
      ],
      "metadata": {
        "id": "Yk4leRcQLjLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences_Example = [\"A E T C Z A O\", \"S K T Z P\"]\n",
        "sequences_Example = [re.sub(r\"[UZOB]\", \"X\", sequence) for sequence in sequences_Example]"
      ],
      "metadata": {
        "id": "DjmWhJEiNhxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = tokenizer.batch_encode_plus(sequences_Example, add_special_tokens=True, padding=True, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "_C2epz_2PTvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = ids['input_ids']\n",
        "attention_mask = ids['attention_mask']"
      ],
      "metadata": {
        "id": "bs4d8RxUPUtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.inference_mode():\n",
        "    embedding = model(input_ids, attention_mask=attention_mask)[0]"
      ],
      "metadata": {
        "id": "tm_6SHIwPXTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding.shape"
      ],
      "metadata": {
        "id": "E18wgWhWP39X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Эти эмбеддинги можно использовать для разных целей. Можно пытаться предсказывать функцию, локацию белка, определять вторичную структуру или положение относительно мембраны.\n",
        "\n",
        "Но боюсь, что времени производить много эмбеддингов сейчас нет."
      ],
      "metadata": {
        "id": "x4Ti5rYmPSbB"
      }
    }
  ]
}