{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cah-qVTIFVXM"
      },
      "source": [
        "# Семинар 1: Эмбеддинги слов\n",
        "\n",
        "Сегодня мы поиграем с эмбеддингами слов: обучим наши собственные маленькие эмбеддинги, загрузим один из них из gensim и используем его для визуализации текстовых корпусов.\n",
        "\n",
        "Если хотите запустить локально, то не забудьте\n",
        "\n",
        "```bash\n",
        "pip install --upgrade nltk gensim bokeh\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qmd4u4YLFVXO",
        "outputId": "712690f6-ea0f-44a2-a162-3ad1e3628133"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-13 10:50:31--  https://www.dropbox.com/s/obaitrix9jyu84r/quora.txt?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.6.18, 2620:100:601f:18::a27d:912\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.6.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/dl/obaitrix9jyu84r/quora.txt [following]\n",
            "--2024-02-13 10:50:31--  https://www.dropbox.com/s/dl/obaitrix9jyu84r/quora.txt\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uccd6c748b229f27f9219091cdfe.dl.dropboxusercontent.com/cd/0/get/CNOKMiURB00DDJG-fH727cbzxZXSu8pA1_cRWJ_voTpWfBwNbIDD_16unZc_UE2_SFTSN80MsXO-VM4K0hoTRLqQltVm1WVQR3H8G_oo1iJG6VEZsYktxH033b5G18n0yxo/file?dl=1# [following]\n",
            "--2024-02-13 10:50:32--  https://uccd6c748b229f27f9219091cdfe.dl.dropboxusercontent.com/cd/0/get/CNOKMiURB00DDJG-fH727cbzxZXSu8pA1_cRWJ_voTpWfBwNbIDD_16unZc_UE2_SFTSN80MsXO-VM4K0hoTRLqQltVm1WVQR3H8G_oo1iJG6VEZsYktxH033b5G18n0yxo/file?dl=1\n",
            "Resolving uccd6c748b229f27f9219091cdfe.dl.dropboxusercontent.com (uccd6c748b229f27f9219091cdfe.dl.dropboxusercontent.com)... 162.125.6.15, 2620:100:601c:15::a27d:60f\n",
            "Connecting to uccd6c748b229f27f9219091cdfe.dl.dropboxusercontent.com (uccd6c748b229f27f9219091cdfe.dl.dropboxusercontent.com)|162.125.6.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 33813903 (32M) [application/binary]\n",
            "Saving to: ‘./quora.txt’\n",
            "\n",
            "./quora.txt         100%[===================>]  32.25M   185MB/s    in 0.2s    \n",
            "\n",
            "2024-02-13 10:50:32 (185 MB/s) - ‘./quora.txt’ saved [33813903/33813903]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# download the data:\n",
        "!wget https://www.dropbox.com/s/obaitrix9jyu84r/quora.txt?dl=1 -O ./quora.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "xPAovGZUFVXQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "with open(\"./quora.txt\", encoding=\"utf-8\") as file:\n",
        "    data = list(file)\n",
        "\n",
        "data[50]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Do5El2V9FVXR"
      },
      "source": [
        "__Токенизация:__ стандартный первый шаг в NLP пайплайне.\n",
        "Текст, с которым мы работаем, представлен сырой: со всеми знаками препинания, смайликами и так далее, поэтому простой str.split не подойдет.\n",
        "\n",
        "Используем __`nltk`__ - библиотеку для решения многих задач NLP, таких как токенизация, стемминг или определение частей речи."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "UuHFEEIeFVXR",
        "outputId": "3a3fa4a1-5a91-4ff7-ed6f-82253169774b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-598654d62d9b>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordPunctTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "tokenizer = WordPunctTokenizer()\n",
        "\n",
        "print(tokenizer.tokenize(data[50]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "EjPN0aUgFVXS"
      },
      "outputs": [],
      "source": [
        "# TASK: все в нижнем регистре и извлекаем лексемы с помощью токенизатора.\n",
        "# data_tok должен быть списком со списков токенов для каждой строки в данных.\n",
        "\n",
        "data_tok = # YOUR CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7RMykPeDFVXS"
      },
      "outputs": [],
      "source": [
        "assert all(isinstance(row, (list, tuple)) for row in data_tok), \"преобразуйте каждую строку в список токенов (строк).\"\n",
        "assert all(all(isinstance(tok, str) for tok in row) for row in data_tok), \"преобразуйте каждую строку в список токенов (строк).\"\n",
        "is_latin = lambda tok: all('a' <= x.lower() <= 'z' for x in tok)\n",
        "assert all(map(lambda l: not is_latin(l) or l.islower(), map(' '.join, data_tok))), \"пожалуйста, убедитесь, что данные написаны в нижнем регистре\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JlP24asFVXT"
      },
      "outputs": [],
      "source": [
        "print([' '.join(row) for row in data_tok[:2]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3Xp5qCMFVXT"
      },
      "source": [
        "__Word vectors:__ кроме упомянутых на лекции способов моделей есть *GloVe* с разными целевыми функциями и *fasttext*, который использует модели на уровне символов для обучения вкраплений слов. Все методы довольно похожи, fasttext позволяет оперировать неизвестными словами, разбивая их на части.\n",
        "\n",
        "Выбор огромен, поэтому начнем с малого: __gensim__ - еще одна библиотека NLP, в которой представлено множество векторных моделей, в том числе и word2vec."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Yr2vxSqkFVXU"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "model = Word2Vec(data_tok,\n",
        "                 vector_size=32,      # embedding vector size\n",
        "                 min_count=5,  # consider words that occured at least 5 times\n",
        "                 window=5).wv  # define context as a 5-word window around the target word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfgkSYoaFVXU"
      },
      "outputs": [],
      "source": [
        "# now you can get word vectors !\n",
        "model.get_vector('anything')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8XW8k60FVXV"
      },
      "outputs": [],
      "source": [
        "# or query similar words directly. Go play with it!\n",
        "model.most_similar('bread')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwilWUXwFVXV"
      },
      "source": [
        "### Претренированные модели\n",
        "\n",
        "Как вы заметили, процесс не быстрый. А теперь представьте себе обучение эмбеддингов слов на гигабайтах текста: статьях в Википедии или сообщениях в X.\n",
        "\n",
        "К счастью, существует куча предобученных моделей."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "DpaSjT94FVXV"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "model = api.load('glove-twitter-100')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmw7brAZFVXW"
      },
      "outputs": [],
      "source": [
        "model.most_similar(positive=[\"coder\", \"money\"], negative=[\"brain\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRk0k-BdFVXW"
      },
      "source": [
        "### Визуализация векторов\n",
        "\n",
        "Один из способов проверить, насколько хороши наши векторы - отобразить их на графике.\n",
        "\n",
        "Заодно вспомним о  методах уменьшения _размерности_.\n",
        "\n",
        "Построим scatterplot 1000 наиболее часто встречающихся слов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jz2f-QKRFVXW"
      },
      "outputs": [],
      "source": [
        "words = model.index_to_key[:1000]\n",
        "\n",
        "print(words[::100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgOULGf4FVXX"
      },
      "outputs": [],
      "source": [
        "# for each word, compute it's vector with model\n",
        "word_vectors = # YOUR CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "mXFZEWRwFVXX"
      },
      "outputs": [],
      "source": [
        "assert isinstance(word_vectors, np.ndarray)\n",
        "assert word_vectors.shape == (len(words), 100)\n",
        "assert np.isfinite(word_vectors).all()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnWT3h_DFVXX"
      },
      "source": [
        "#### PCA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6goNIejPFVXX"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Отобразите векторы слов на двумерную плоскость с помощью PCA. Используйте старый добрый sklearn api (fit, transform)\n",
        "# после этого нормализуйте векторы, чтобы убедиться, что у них нулевое среднее и единичная дисперсия\n",
        "word_vectors_pca = # YOUR CODE\n",
        "\n",
        "# and maybe MORE OF YOUR CODE here :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_NJJkuHcFVXX"
      },
      "outputs": [],
      "source": [
        "assert word_vectors_pca.shape == (len(word_vectors), 2), \"для каждого слова должен существовать 2d-вектор\"\n",
        "assert max(abs(word_vectors_pca.mean(0))) < 1e-5, \"точки должны быть центрированы\"\n",
        "assert max(abs(1.0 - word_vectors_pca.std(0))) < 1e-2, \"точки должны иметь единичную дисперсию\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5LdN9hLFVXX"
      },
      "source": [
        "#### Рисуем!\n",
        "\n",
        "В этот раз интерактивные графики с библиотекой bokeh."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEcm6-z7FVXY"
      },
      "outputs": [],
      "source": [
        "import bokeh.models as bm, bokeh.plotting as pl\n",
        "from bokeh.io import output_notebook\n",
        "output_notebook()\n",
        "\n",
        "def draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n",
        "                 width=600, height=400, show=True, **kwargs):\n",
        "    \"\"\" draws an interactive plot for data points with auxilirary info on hover \"\"\"\n",
        "    if isinstance(color, str): color = [color] * len(x)\n",
        "    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n",
        "\n",
        "    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n",
        "    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n",
        "\n",
        "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
        "    if show: pl.show(fig)\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwgI6pmdFVXY"
      },
      "outputs": [],
      "source": [
        "draw_vectors(word_vectors_pca[:, 0], word_vectors_pca[:, 1], token=words)\n",
        "\n",
        "# наведите на точки курсор мыши и посмотрите, сможете ли вы назвать как-то кластеры"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5s06ePgkFVXY"
      },
      "source": [
        "### t-SNE\n",
        "PCA хорош, но он строго линеен и, следовательно, способен отразить только грубую высокоуровневую структуру данных.\n",
        "\n",
        "Если забыли, то вот небольшой материал для повторения: __[more on TSNE](https://distill.pub/2016/misread-tsne/)__."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAjahujlFVXY"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# отобразите векторы слов на двумерную плоскость с помощью TSNE. подсказка: не паникуйте, на подгонку может уйти минута или две.\n",
        "# нормализуйте их так же, как и в pca\n",
        "\n",
        "\n",
        "word_tsne = #YOUR CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "scrolled": false,
        "id": "U7kfACvEFVXY"
      },
      "outputs": [],
      "source": [
        "draw_vectors(word_tsne[:, 0], word_tsne[:, 1], color='green', token=words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPYiV5ieFVXY"
      },
      "source": [
        "### Визуализация фраз\n",
        "\n",
        "Эмбеддинги слов также можно использовать для представления коротких фраз. Самый простой способ - взять __среднее__ векторов для всех лексем во фразе с некоторыми весами.\n",
        "\n",
        "Этот трюк полезен для того, чтобы определить, с какими данными вы работаете: найти, есть ли какие-либо выбросы, кластеры или какие-то артефакты.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "rLFcGTVyFVXY"
      },
      "outputs": [],
      "source": [
        "def get_phrase_embedding(phrase):\n",
        "    \"\"\"\n",
        "    Преобразуйте фразу в вектор, объединив в нем вкрапления слов. См. описание выше.\n",
        "    \"\"\"\n",
        "    # 1. переведите фразу в нижний регистре\n",
        "    # 2. токенизируйте фразу\n",
        "    # 3. усредните векторы для всех слов в токенизированной фразе\n",
        "    # пропускаем слова, которых нет в словарe модели\n",
        "    # если все слова отсутствуют в словарe, возвращаются нули\n",
        "\n",
        "    vector = np.zeros([model.vector_size], dtype='float32')\n",
        "\n",
        "    # YOUR CODE\n",
        "\n",
        "    return vector\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jvTsCKgiFVXY"
      },
      "outputs": [],
      "source": [
        "vector = get_phrase_embedding(\"I'm very sure. This never happened to me before...\")\n",
        "\n",
        "assert np.allclose(vector[::10],\n",
        "                   np.array([ 0.31807372, -0.02558171,  0.0933293 , -0.1002182 , -1.0278689 ,\n",
        "                             -0.16621883,  0.05083408,  0.17989802,  1.3701859 ,  0.08655966],\n",
        "                              dtype=np.float32))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "VoRppE3dFVXZ"
      },
      "outputs": [],
      "source": [
        "# рассмотрим только ~5k фраз\n",
        "chosen_phrases = data[::len(data) // 1000]\n",
        "\n",
        "# рассчитайте векторы фраз\n",
        "phrase_vectors = # YOUR CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "cuy8KAIyFVXZ"
      },
      "outputs": [],
      "source": [
        "assert isinstance(phrase_vectors, np.ndarray) and np.isfinite(phrase_vectors).all()\n",
        "assert phrase_vectors.shape == (len(chosen_phrases), model.vector_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "d7vpsnwzFVXZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "outputId": "ad08544b-bfb3-4042-d520-59b34e25ba43"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'TSNE' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4785e53535a6>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# don't forget to normalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mphrase_vectors_2d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mphrase_vectors_2d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mphrase_vectors_2d\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mphrase_vectors_2d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mphrase_vectors_2d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'TSNE' is not defined"
          ]
        }
      ],
      "source": [
        "# отобразите векторы в двумерное пространство с помощью pca, tsne или другого выбранного вами метода\n",
        "# не забудьте нормализовать\n",
        "\n",
        "phrase_vectors_2d = TSNE().fit_transform(phrase_vectors)\n",
        "\n",
        "phrase_vectors_2d = (phrase_vectors_2d - phrase_vectors_2d.mean(axis=0)) / phrase_vectors_2d.std(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "MXqdVggLFVXZ"
      },
      "outputs": [],
      "source": [
        "draw_vectors(phrase_vectors_2d[:, 0], phrase_vectors_2d[:, 1],\n",
        "             phrase=[phrase[:50] for phrase in chosen_phrases],\n",
        "             radius=20,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOMCzPRVFVXZ"
      },
      "source": [
        "Finally, let's build a simple \"similar question\" engine with phrase embeddings we've built."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1_kmm3fyFVXZ"
      },
      "outputs": [],
      "source": [
        "# вычислите векторы для всех строк\n",
        "data_vectors = np.array([get_phrase_embedding(l) for l in data])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zA0zmyb9FVXa"
      },
      "outputs": [],
      "source": [
        "def find_nearest(query, k=10):\n",
        "    \"\"\"\n",
        "    вернуть k наиболее похожих строк из данных, отсортированных от наиболее до наименее похожих\n",
        "    сходство должно измеряться как косинус между векторами запроса и строки\n",
        "    можно использовать глобальные переменные: data и data_vectors. см. также: np.argpartition, np.argsort\n",
        "    \"\"\"\n",
        "    # YOUR CODE\n",
        "\n",
        "    return <YOUR CODE: top-k lines starting from most similar>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "DYajXQYyFVXa"
      },
      "outputs": [],
      "source": [
        "results = find_nearest(query=\"How do i enter the matrix?\", k=10)\n",
        "\n",
        "print(''.join(results))\n",
        "\n",
        "assert len(results) == 10 and isinstance(results[0], str)\n",
        "assert results[0] == 'How do I get to the dark web?\\n'\n",
        "assert results[3] == 'What can I do to save the world?\\n'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0tIe4pH5FVXa"
      },
      "outputs": [],
      "source": [
        "find_nearest(query=\"How does Trump?\", k=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5IDqnANlFVXa"
      },
      "outputs": [],
      "source": [
        "find_nearest(query=\"Why don't i ask a question myself?\", k=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "PqVyIwblFVXa"
      },
      "source": [
        "__Что еще?__\n",
        "* Посмотрите, что еще есть в model zoo: `gensim.downloader.info()`\n",
        "* Попробуйте [FastText](https://github.com/facebookresearch/fastText)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}